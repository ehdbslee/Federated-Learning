{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WorkStation\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root='C:/Users/WorkStation/Desktop/Git_Repositories/Dataset', train = True, transform = transforms.ToTensor(), download = True)\n",
    "test_dataset = datasets.MNIST(root='C:/Users/WorkStation/Desktop/Git_Repositories/Dataset', train = False, transform = transforms.ToTensor(), download = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Functions(for all)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args):    \n",
    "    ldt = pd.read_csv(f\"FL(mining_data)/parameter_data/param_data_{args.num_clients}.csv\")\n",
    "    ldt = ldt.values\n",
    "    ldt = np.delete(ldt, 0, 1)\n",
    "    return ldt\n",
    "\n",
    "def data_split(train_dataset, args):\n",
    "    dataset_list=[]\n",
    "\n",
    "    num_data_arr = np.ones((args.num_clients,), dtype=int) * int(len(train_dataset)/args.num_clients)\n",
    "    dataset_list = torch.utils.data.random_split(train_dataset, num_data_arr)\n",
    "    return dataset_list\n",
    "\n",
    "def list_np_tensor(param_data):\n",
    "    data = np.concatenate((param_data['net.0.weight'].cpu().numpy(),np.expand_dims(param_data['net.0.bias'].cpu().numpy(), axis=1)),axis=1)\n",
    "    data = torch.Tensor(data)\n",
    "    return data\n",
    "\n",
    "def plot_loss(FL_round_loss, FL_round_acc, round_losses, round_acces, avg_round_losses, avg_round_acces):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(FL_round_loss, label = 'FL')\n",
    "    plt.plot(round_losses, label = 'FL+AE')\n",
    "    plt.plot(avg_round_losses, label = 'FL+AE(w_avg)')\n",
    "    plt.xlabel('Rounds')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(FL_round_acc, label = 'FL')\n",
    "    plt.plot(round_acces, label = 'FL+AE')\n",
    "    plt.plot(avg_round_acces, label = 'FL+AE(w_avg)')\n",
    "    plt.xlabel('Rounds')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder = nn.Sequential(nn.Linear(7851, 785),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(785, 50),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(50, self.hidden_dim)\n",
    "                                    )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        output = self.encoder(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.decoder = nn.Sequential(nn.Linear(self.hidden_dim, 50),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(50, 785),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(785, 7851)\n",
    "                                    )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        output = self.decoder(x)\n",
    "        return output\n",
    "        \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(28*28, 10)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28)\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **FL Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FL_define_client(partition, r, num, args):\n",
    "    train_loader = DataLoader(partition['train'][num], batch_size=args.batch_size, shuffle=True)\n",
    "    \n",
    "    if r == 0:\n",
    "        net = Net().to(device)\n",
    "    else:\n",
    "        net = torch.load(args.INIT_PATH)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=args.lr)\n",
    "\n",
    "    return net, train_loader, criterion, optimizer\n",
    "\n",
    "def FL_train(net, train_loader, criterion, optimizer, args):\n",
    "\n",
    "    for epoch in range(args.num_epochs):        \n",
    "        for images, labels in iter(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            outputs = net(images)  \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    torch.save(net, args.NET_PATH)\n",
    "    return net\n",
    "\n",
    "def FL_Fed_AVG(model, client_model_list, args):\n",
    "    model.train()\n",
    "    local_weights = []\n",
    "\n",
    "    for i in range(args.num_clients):\n",
    "        local_weights.append(copy.deepcopy(client_model_list[i].state_dict()))\n",
    "    w_avg = copy.deepcopy(local_weights[0]) #type of w_avg : <class 'collections.OrderedDict'>\n",
    "\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1,len(local_weights)):\n",
    "            w_avg[key] += local_weights[i][key]\n",
    "\n",
    "        w_avg[key] = torch.div(w_avg[key], len(local_weights))\n",
    "\n",
    "    global_weights = w_avg\n",
    "    model.load_state_dict(global_weights)\n",
    "    torch.save(model, args.INIT_PATH)\n",
    "\n",
    "    return model\n",
    "\n",
    "def FL_test(model, partition, criterion, args):\n",
    "    test_loader = DataLoader(partition['test'], batch_size=args.batch_size, shuffle=True)\n",
    "    model.eval()\n",
    "        \n",
    "    correct = 0 # 정답을 맞힌 이미지 개수\n",
    "    total = 0 # 전체 이미지 개수\n",
    "    loss = 0\n",
    "    round_accuracy = 0\n",
    "    round_loss = 0\n",
    "\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss += criterion(outputs, targets).item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "    \n",
    "    round_loss = loss / len(test_loader)\n",
    "    round_accuracy = (correct / total) * 100.0\n",
    "    \n",
    "    return round_loss, round_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FL_experiment(partition, args):\n",
    "    model = Net().to(device)\n",
    "    torch.save(model, args.INIT_PATH)\n",
    "\n",
    "    round_losses = []\n",
    "    round_acces = []\n",
    "\n",
    "    for r in range(args.num_rounds):\n",
    "        client_model_list = []\n",
    "        \n",
    "        for num in range(args.num_clients):\n",
    "            setattr(args, 'NET_PATH', f'FL_model/client = {args.num_clients}/client_{num}.pth')\n",
    "            \n",
    "            net, train_loader, criterion, optimizer = FL_define_client(partition, r, num, args)\n",
    "            net = FL_train(net, train_loader, criterion, optimizer, args)\n",
    "            client_model_list.append(net)\n",
    "\n",
    "        model = FL_Fed_AVG(model, client_model_list, args)\n",
    "        round_loss, round_acc = FL_test(model, partition, criterion, args)\n",
    "\n",
    "        if r%50 == 0 or r==(args.num_rounds-1):\n",
    "            print(f'[Round {r}]  Round Loss: {round_loss}  Round Acc: {round_acc:.2f}')\n",
    "\n",
    "        round_losses.append(round_loss)\n",
    "        round_acces.append(round_acc)\n",
    "        \n",
    "    return round_losses, round_acces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **FL+AE Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_client(partition, global_weights, r, num, args):\n",
    "    train_loader = DataLoader(partition['train'][num], batch_size=args.batch_size, shuffle=True)\n",
    "    \n",
    "    if r == 0:\n",
    "        net = Net().to(device)\n",
    "\n",
    "    else:\n",
    "        net = Net().to(device)        \n",
    "        net.load_state_dict(global_weights)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=args.lr)\n",
    "\n",
    "    return net, train_loader, criterion, optimizer\n",
    "\n",
    "def train(net, train_loader, criterion, optimizer, args):\n",
    "    net.train()\n",
    "\n",
    "    for epoch in range(args.num_epochs):        \n",
    "        for images, labels in iter(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            outputs = net(images)  \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return net\n",
    "\n",
    "def client_encode(net, r, encoder):\n",
    "    round_tensor = torch.tensor(r).view(-1, 1).to(device)\n",
    "\n",
    "    param_data = copy.deepcopy(net.state_dict())\n",
    "    tensor_data = list_np_tensor(param_data).view(-1,7850).to(device)\n",
    "    dt_r = torch.cat((tensor_data, round_tensor), dim=1)\n",
    "    encoded_data = encoder(dt_r)\n",
    "    return encoded_data\n",
    "\n",
    "def Dec_Fed_AVG(model, decoder, client_enc_data_list, args):\n",
    "    model.train()\n",
    "    decoded_data_list = []\n",
    "    global_weights = OrderedDict()\n",
    "\n",
    "    for i in range(args.num_clients):\n",
    "        client_enc_data_list[i] = client_enc_data_list[i].to(device)\n",
    "        out = decoder(client_enc_data_list[i])[:,:-1].view((-1, 785)).cpu().detach().numpy()    \n",
    "        decoded_data_list.append(out)\n",
    "\n",
    "    w_avg = np.mean(decoded_data_list, axis=0)\n",
    "    global_weights['net.0.weight'] = torch.tensor(w_avg[:,:784]).clone().detach()\n",
    "    global_weights['net.0.bias'] = torch.tensor(w_avg[:,784:].squeeze()).clone().detach()\n",
    "\n",
    "    model.load_state_dict(global_weights)\n",
    "\n",
    "    return model, global_weights\n",
    "\n",
    "def test(model, partition, criterion, args):\n",
    "    test_loader = DataLoader(partition['test'], batch_size=args.batch_size, shuffle=True)\n",
    "    model.eval()\n",
    "        \n",
    "    correct = 0 \n",
    "    total = 0 \n",
    "    loss = 0\n",
    "    round_accuracy = 0\n",
    "    round_loss = 0\n",
    "\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss += criterion(outputs, targets).item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "    \n",
    "    round_loss = loss / len(test_loader)\n",
    "    round_accuracy = (correct / total) * 100.0\n",
    "    \n",
    "    return round_loss, round_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "    encoder = Encoder(args.hidden_dim)\n",
    "    decoder = Decoder(args.hidden_dim)\n",
    "    model = Net()\n",
    "    encoder.load_state_dict(torch.load(args.enc_path))\n",
    "    decoder.load_state_dict(torch.load(args.dec_path))\n",
    "\n",
    "    encoder, decoder, model = encoder.to(device), decoder.to(device), model.to(device)\n",
    "    \n",
    "    global_weights = OrderedDict()\n",
    "    round_losses = []\n",
    "    round_acces = []\n",
    "\n",
    "\n",
    "    for r in range(args.num_rounds):\n",
    "        client_enc_data_list = []\n",
    "\n",
    "        for num in range(args.num_clients):\n",
    "            net, train_loader, criterion, optimizer = define_client(partition, global_weights, r, num, args)\n",
    "            net = train(net, train_loader, criterion, optimizer, args)\n",
    "            client_enc_data = client_encode(net, r, encoder)\n",
    "            client_enc_data_list.append(client_enc_data)\n",
    "\n",
    "        model, global_weights = Dec_Fed_AVG(model, decoder, client_enc_data_list, args)\n",
    "        round_loss, round_acc = test(model, partition, criterion, args)\n",
    "\n",
    "        if r%50 == 0 or r == (args.num_rounds-1):\n",
    "            print(f'[Round: {r}]  Round Loss: {round_loss}  Round Acc: {round_acc:.2f}')\n",
    "\n",
    "        round_losses.append(round_loss)\n",
    "        round_acces.append(round_acc)\n",
    "        \n",
    "    return round_losses, round_acces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(loss, acc, num_data=None, avg=False):\n",
    "    dtf_loss, dtf_acc = pd.DataFrame(loss), pd.DataFrame(acc)\n",
    "\n",
    "    if num_data == None:\n",
    "        dtf_loss.to_csv(f\"paper_data/[loss]Normal_FL.csv\")\n",
    "        dtf_acc.to_csv(f\"paper_data/[acc]Normal_FL.csv\")\n",
    "        \n",
    "    else:\n",
    "        if avg==False:\n",
    "            dtf_loss.to_csv(f\"paper_data/[loss]Non_Avg_FL_{num_data}.csv\")\n",
    "            dtf_acc.to_csv(f\"paper_data/[acc]Non_Avg_FL_{num_data}.csv\")\n",
    "        else:\n",
    "            dtf_loss.to_csv(f\"paper_data/[loss]Avg_FL_{num_data}.csv\")\n",
    "            dtf_acc.to_csv(f\"paper_data/[acc]Avg_FL_{num_data}.csv\")\n",
    "    print('save loss & acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Normal FL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round 0]  Round Loss: 0.7766053175926209  Round Acc: 84.96\n",
      "[Round 50]  Round Loss: 0.2635316574573517  Round Acc: 92.67\n",
      "[Round 100]  Round Loss: 0.2630053900927305  Round Acc: 92.82\n",
      "[Round 150]  Round Loss: 0.2662125315517187  Round Acc: 92.80\n",
      "[Round 200]  Round Loss: 0.2698613403737545  Round Acc: 92.92\n",
      "[Round 250]  Round Loss: 0.27387715376913546  Round Acc: 92.84\n",
      "[Round 300]  Round Loss: 0.27760546617209914  Round Acc: 92.89\n",
      "[Round 350]  Round Loss: 0.2811790708452463  Round Acc: 92.89\n",
      "[Round 400]  Round Loss: 0.28480591617524625  Round Acc: 92.90\n",
      "[Round 450]  Round Loss: 0.2881507502496243  Round Acc: 92.91\n",
      "[Round 499]  Round Loss: 0.29114479281008243  Round Acc: 92.88\n",
      "save loss & acc\n"
     ]
    }
   ],
   "source": [
    "seed = 1228\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.exp_name = 'final'\n",
    "\n",
    "args.num_epochs = 5\n",
    "args.num_rounds = 500\n",
    "args.num_clients = 3\n",
    "args.lr = 0.0001\n",
    "\n",
    "args.batch_size = 100\n",
    "\n",
    "args.NET_PATH = ''\n",
    "args.INIT_PATH = f'FL_model/client = {args.num_clients}/initialize_weight.pth'\n",
    "\n",
    "train_set_list = data_split(train_dataset, args)\n",
    "partition = {'train':train_set_list, 'test':test_dataset}\n",
    "\n",
    "FL_round_loss, FL_round_acc = FL_experiment(partition, args)\n",
    "save_list(FL_round_loss, FL_round_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **[FL+AE] Non_Avg vs Avg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== num_data: 250 =====\n",
      "avg: False\n",
      "[Round: 0]  Round Loss: 0.6027184376120567  Round Acc: 86.74\n",
      "[Round: 50]  Round Loss: 0.3913286290317774  Round Acc: 91.82\n",
      "[Round: 100]  Round Loss: 0.8110113221406937  Round Acc: 90.89\n"
     ]
    }
   ],
   "source": [
    "seed = 1228\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "# ====== Model Capacity ====== #\n",
    "args.num_epochs = 5\n",
    "args.num_rounds = 500\n",
    "args.num_clients = 3\n",
    "args.lr = 0.0001\n",
    "\n",
    "args.batch_size = 100\n",
    "args.hidden_dim = 15\n",
    "args.num_data = 100\n",
    "\n",
    "args.avg = False\n",
    "\n",
    "name_var1 = 'num_data'\n",
    "name_var2 = 'avg'\n",
    "\n",
    "list_var1 = [50, 500]\n",
    "list_var2 = [False, True]\n",
    "\n",
    "\n",
    "for var1 in list_var1:\n",
    "    print(f\"===== {name_var1}: {var1*5} =====\")\n",
    "    setattr(args, name_var1, var1)\n",
    "\n",
    "    for var2 in list_var2:\n",
    "        print(f\"{name_var2}: {var2}\") \n",
    "        setattr(args, name_var2, var2)\n",
    "  \n",
    "        train_set_list = data_split(train_dataset, args)\n",
    "        partition = {'train':train_set_list, 'test':test_dataset}\n",
    "\n",
    "        if var2 == False:\n",
    "            args.enc_path = f\"AE_model/client = {args.num_clients}/enc_num_data_{var1}.pth\"\n",
    "            args.dec_path = f\"AE_model/client = {args.num_clients}/dec_num_data_{var1}.pth\"\n",
    "            round_losses, round_acces = experiment(partition, args)\n",
    "            save_list(round_losses, round_acces, num_data=var1, avg=var2)\n",
    "\n",
    "        else:\n",
    "            args.enc_path = f\"FL(mining_data)/w_avg_AE_model/client = {args.num_clients}/enc_num_data_{var1}.pth\"\n",
    "            args.dec_path = f\"FL(mining_data)/w_avg_AE_model/client = {args.num_clients}/dec_num_data_{var1}.pth\"\n",
    "            avg_round_losses, avg_round_acces = experiment(partition, args)\n",
    "            save_list(avg_round_losses, avg_round_acces, num_data=var1, avg=var2)\n",
    "\n",
    "    plot_loss(FL_round_loss, FL_round_acc, round_losses, round_acces, avg_round_losses, avg_round_acces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(num_data=None, avg=False): \n",
    "    if num_data == None:\n",
    "        ldt_loss = pd.read_csv(f\"paper_data/[loss]Normal_FL.csv\")\n",
    "        ldt_acc = pd.read_csv(f\"paper_data/[acc]Normal_FL.csv\")\n",
    "        \n",
    "    else:\n",
    "        if avg==False:\n",
    "            ldt_loss = pd.read_csv(f\"paper_data/[loss]Non_Avg_FL_{num_data}.csv\")\n",
    "            ldt_acc = pd.read_csv(f\"paper_data/[acc]Non_Avg_FL_{num_data}.csv\")\n",
    "\n",
    "        else:\n",
    "            ldt_loss = pd.read_csv(f\"paper_data/[loss]Avg_FL_{num_data}.csv\")\n",
    "            ldt_acc = pd.read_csv(f\"paper_data/[acc]Avg_FL_{num_data}.csv\")\n",
    "\n",
    "    ldt_loss, ldt_acc = np.delete(ldt_loss.values, 0, 1), np.delete(ldt_acc.values, 0, 1)\n",
    "    return ldt_loss, ldt_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34349495]\n",
      " [0.30909618]\n",
      " [0.31210935]\n",
      " [0.31298341]\n",
      " [0.31353626]] [[91.5 ]\n",
      " [92.65]\n",
      " [92.63]\n",
      " [92.63]\n",
      " [92.62]]\n",
      "[[0.34349495]\n",
      " [0.30909618]\n",
      " [0.31210935]\n",
      " [0.31298341]\n",
      " [0.31353626]] [[91.5 ]\n",
      " [92.65]\n",
      " [92.63]\n",
      " [92.63]\n",
      " [92.62]]\n",
      "\n",
      "\n",
      "[[0.29874221]\n",
      " [0.28698858]\n",
      " [0.28863793]\n",
      " [0.28930337]\n",
      " [0.28981996]] [[91.92]\n",
      " [92.72]\n",
      " [92.68]\n",
      " [92.67]\n",
      " [92.67]]\n",
      "[[0.29874221]\n",
      " [0.28698858]\n",
      " [0.28863793]\n",
      " [0.28930337]\n",
      " [0.28981996]] [[91.92]\n",
      " [92.72]\n",
      " [92.68]\n",
      " [92.67]\n",
      " [92.67]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Normal_FL_loss, Normal_FL_acc = load_data()\n",
    "print(Normal_FL_loss, Normal_FL_acc)\n",
    "\n",
    "for var1 in list_var1:\n",
    "    Non_Avg_loss, Non_Avg_acc = load_data(num_data=var1, avg=False)\n",
    "    Avg_loss, Avg_acc = load_data(num_data=var1, avg=True)\n",
    "    print(Non_Avg_loss, Non_Avg_acc)\n",
    "    print(Avg_loss, Avg_acc)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36b28c691283740da8fac429d25fdbdfa5fc153c522ef0a5c3318a9b3b280dc3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
