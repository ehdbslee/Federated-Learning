{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "\n",
    "from collections import OrderedDict\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(args):    \n",
    "    import os\n",
    "\n",
    "    MODEL_DIR = f\"AE_model/client = {args.num_clients}\"\n",
    "    try:\n",
    "        os.makedirs(MODEL_DIR)\n",
    "    except FileExistsError:\n",
    "        print('Directories not created because they already exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "    # ===== w_data ===== #\n",
    "    df = pd.read_csv(f\"Final/parameter_data/param_data_{args.num_clients}.csv\")\n",
    "    data = torch.Tensor(np.delete(df.values, 0, 1))\n",
    "\n",
    "    # ===== MNIST data ===== #\n",
    "    train_dataset = datasets.MNIST(root='MNIST_data/', train = True, transform = transforms.ToTensor(), download = True)\n",
    "    val_dataset = datasets.MNIST(root='MNIST_data/', train = False, transform = transforms.ToTensor(), download = True)\n",
    "\n",
    "    train_dataset, _ = torch.utils.data.random_split(train_dataset, [10000, 50000])\n",
    "    val_dataset, _ = torch.utils.data.random_split(val_dataset, [2000, 8000])\n",
    "    return data, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder = nn.Sequential(nn.Linear(7851, 785),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(785, 50),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(50, self.hidden_dim)\n",
    "                                    )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        output = self.encoder(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.decoder = nn.Sequential(nn.Linear(self.hidden_dim, 50),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(50, 785),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(785, 7851)\n",
    "                                    )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        output = self.decoder(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(28*28, 10)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28)\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, net, partition, optimizer, criterion, args):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    net.train()\n",
    "    train_loader = torch.utils.data.DataLoader(partition['train'], batch_size=args.batch_size*3)\n",
    "    mnist_train_loader = torch.utils.data.DataLoader(partition['mnist_train'], batch_size=args.mnist_batch_size)\n",
    "    \n",
    "    train_loss = 0\n",
    "    for train_data in train_loader:\n",
    "        train_data = train_data.to(device)\n",
    "        ae_out = decoder(encoder(train_data))\n",
    "        w_avg_hat = torch.mean(ae_out.view(args.num_clients, -1, 7851), dim=0)[:,:-1].view(-1,785)\n",
    "\n",
    "        global_weights = OrderedDict()\n",
    "        global_weights['net.0.weight'] = w_avg_hat[:,:784]\n",
    "        global_weights['net.0.bias'] = w_avg_hat[:,784:].squeeze()\n",
    "        net.load_state_dict(global_weights)\n",
    "\n",
    "        #iter_loss = 0\n",
    "        for mnist_data, mnist_label in mnist_train_loader:\n",
    "            mnist_data, mnist_label = mnist_data.to(device), mnist_label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            net_out = net(mnist_data)\n",
    "            loss = criterion(net_out, mnist_label)\n",
    "            train_loss += loss.item()\n",
    "            iter_loss += loss.item()\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #print(iter_loss/len(mnist_train_loader))\n",
    "\n",
    "    train_loss /= (len(train_loader)*len(mnist_train_loader))\n",
    "    return encoder, decoder, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(encoder, decoder, net, partition, criterion, args):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    val_loader = torch.utils.data.DataLoader(partition['val'], batch_size=args.batch_size*3, shuffle=True)\n",
    "    mnist_val_loader = torch.utils.data.DataLoader(partition['mnist_val'], batch_size=args.mnist_batch_size, shuffle=True)\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "\n",
    "        for val_data in val_loader:\n",
    "\n",
    "            val_data = val_data.to(device)\n",
    "\n",
    "            ae_out = decoder(encoder(val_data))\n",
    "            val_w_avg_hat = torch.mean(ae_out.view(args.num_clients, -1, 7851), dim=0)[:,:-1].view(-1,785)\n",
    "\n",
    "            global_weights = OrderedDict()\n",
    "            global_weights['net.0.weight'] = val_w_avg_hat[:,:784]\n",
    "            global_weights['net.0.bias'] = val_w_avg_hat[:,784:].squeeze()\n",
    "            net.load_state_dict(global_weights)\n",
    "\n",
    "            for mnist_val_data, mnist_val_label in mnist_val_loader:\n",
    "                mnist_val_data, mnist_val_label = mnist_val_data.to(device), mnist_val_label.to(device)\n",
    "\n",
    "                net_out = net(mnist_val_data)\n",
    "                loss = criterion(net_out , mnist_val_label)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= (len(val_loader)*len(mnist_val_loader))\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "    encoder = Encoder(args.hidden_dim)\n",
    "    decoder = Decoder(args.hidden_dim)\n",
    "    net = Net()\n",
    "    encoder, decoder, net = encoder.to(device), decoder.to(device), net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters())+list(decoder.parameters()), lr = args.lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "\n",
    "        encoder, decoder, train_loss = train(encoder, decoder, net, partition, optimizer, criterion, args)\n",
    "        val_loss = validate(encoder, decoder, net, partition, criterion, args)\n",
    "\n",
    "        #if (epoch+1)%10 == 0:\n",
    "        #    print(f\"[{epoch+1}/{args.num_epochs}]  Train Loss: {train_loss}  Val Loss: {val_loss}\")\n",
    "        print(f\"[{epoch+1}/{args.num_epochs}]  Train Loss: {train_loss}  Val Loss: {val_loss}\")\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "    \n",
    "    return encoder, decoder, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(encoder, decoder, var, args):\n",
    "\n",
    "    if args.reverse == True:\n",
    "        enc_path = f\"min(FL_Loss)_AE_model/client = {args.num_clients}/reverse/enc_num_data_{var}.pth\"\n",
    "        dec_path = f\"min(FL_Loss)_AE_model/client = {args.num_clients}/reverse/dec_num_data_{var}.pth\"\n",
    "    else:  \n",
    "        enc_path = f\"min(FL_Loss)_AE_model/client = {args.num_clients}/enc_num_data_{var}.pth\"\n",
    "        dec_path = f\"min(FL_Loss)_AE_model/client = {args.num_clients}/dec_num_data_{var}.pth\"\n",
    "\n",
    "    torch.save(encoder.state_dict(), enc_path)\n",
    "    torch.save(decoder.state_dict(), dec_path)\n",
    "    print(f\"save model when [{var}] \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(var, name_var, train_losses, val_losses):\n",
    "    plt.title(f\"{name_var} = {var}\")\n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.plot(val_losses, label='val')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories not created because they already exist\n",
      "========== reverse: False ==========\n",
      "[num_data: 100]\n",
      "shape of data: torch.Size([300, 7851])\n",
      "2.3542761325836183\n",
      "2.354082405567169\n",
      "2.3542733550071717\n",
      "2.3538515329360963\n",
      "2.351799488067627\n",
      "2.3532838940620424\n",
      "2.355102574825287\n",
      "2.354480504989624\n",
      "2.3545336604118345\n",
      "2.3543482065200805\n",
      "2.3521487593650816\n",
      "2.355717384815216\n",
      "2.354356038570404\n",
      "2.355105483531952\n",
      "2.352464699745178\n",
      "2.354747176170349\n",
      "2.353770208358765\n",
      "2.3519059300422667\n",
      "2.3573376059532167\n",
      "2.3551472425460815\n",
      "2.355656659603119\n",
      "2.354658281803131\n",
      "2.3538002490997316\n",
      "2.3537532091140747\n",
      "2.3544562578201296\n",
      "2.3541361689567566\n",
      "2.350209367275238\n",
      "2.3523062229156495\n",
      "2.350916123390198\n",
      "2.353810524940491\n",
      "2.352051866054535\n",
      "2.355924165248871\n",
      "2.356825923919678\n",
      "2.413106679916382\n",
      "2.4484464645385744\n",
      "2.451253867149353\n",
      "2.447420620918274\n",
      "2.4473577618598936\n",
      "2.4473183512687684\n",
      "2.44848552942276\n",
      "2.448956286907196\n",
      "2.4493505716323853\n",
      "2.4493940830230714\n",
      "2.4472745656967163\n",
      "2.4478769063949586\n",
      "2.4496517062187193\n",
      "2.45160973072052\n",
      "2.446529722213745\n",
      "2.4506221413612366\n",
      "2.4477417349815367\n",
      "2.4472705245018007\n",
      "2.4490180015563965\n",
      "2.4508753061294555\n",
      "2.4491931557655335\n",
      "2.4491807341575624\n",
      "2.449869966506958\n",
      "2.448374032974243\n",
      "2.4501779079437256\n",
      "2.4465247869491575\n",
      "2.4468809962272644\n",
      "2.4458622217178343\n",
      "2.4461044907569884\n",
      "2.446773874759674\n",
      "2.4487960934638977\n",
      "2.447840702533722\n",
      "2.4515177965164185\n",
      "2.4839944720268248\n",
      "2.5666680574417113\n",
      "2.5680426716804505\n",
      "2.5656267762184144\n",
      "2.565337634086609\n",
      "2.5620843410491942\n",
      "2.5674811482429503\n",
      "2.5639493584632875\n",
      "2.5679381132125854\n",
      "2.569136643409729\n",
      "2.5637284994125364\n",
      "2.564107394218445\n",
      "2.5669809579849243\n",
      "2.5694825649261475\n",
      "[1/10]  Train Loss: 2.428655665665865  Val Loss: 2.5698290914297104\n",
      "2.3542761325836183\n",
      "2.354082405567169\n",
      "2.3542733550071717\n",
      "2.3538515329360963\n",
      "2.351799488067627\n",
      "2.3532838940620424\n",
      "2.355102574825287\n",
      "2.354480504989624\n",
      "2.3545336604118345\n",
      "2.3543482065200805\n",
      "2.3521487593650816\n",
      "2.355717384815216\n",
      "2.354356038570404\n",
      "2.355105483531952\n",
      "2.352464699745178\n",
      "2.354747176170349\n",
      "2.353770208358765\n",
      "2.3519059300422667\n",
      "2.3573376059532167\n",
      "2.3551472425460815\n",
      "2.355656659603119\n",
      "2.354658281803131\n",
      "2.3538002490997316\n",
      "2.3537532091140747\n",
      "2.3544562578201296\n",
      "2.3541361689567566\n",
      "2.350209367275238\n",
      "2.3523062229156495\n",
      "2.350916123390198\n",
      "2.353810524940491\n",
      "2.352051866054535\n",
      "2.355924165248871\n",
      "2.356825923919678\n",
      "2.413106679916382\n",
      "2.4484464645385744\n",
      "2.451253867149353\n",
      "2.447420620918274\n",
      "2.4473577618598936\n",
      "2.4473183512687684\n",
      "2.44848552942276\n",
      "2.448956286907196\n",
      "2.4493505716323853\n",
      "2.4493940830230714\n",
      "2.4472745656967163\n",
      "2.4478769063949586\n",
      "2.4496517062187193\n",
      "2.45160973072052\n",
      "2.446529722213745\n",
      "2.4506221413612366\n",
      "2.4477417349815367\n",
      "2.4472705245018007\n",
      "2.4490180015563965\n",
      "2.4508753061294555\n",
      "2.4491931557655335\n",
      "2.4491807341575624\n",
      "2.449869966506958\n",
      "2.448374032974243\n",
      "2.4501779079437256\n",
      "2.4465247869491575\n",
      "2.4468809962272644\n",
      "2.4458622217178343\n",
      "2.4461044907569884\n",
      "2.446773874759674\n",
      "2.4487960934638977\n",
      "2.447840702533722\n",
      "2.4515177965164185\n",
      "2.4839944720268248\n",
      "2.5666680574417113\n",
      "2.5680426716804505\n",
      "2.5656267762184144\n",
      "2.565337634086609\n",
      "2.5620843410491942\n",
      "2.5674811482429503\n",
      "2.5639493584632875\n",
      "2.5679381132125854\n",
      "2.569136643409729\n",
      "2.5637284994125364\n",
      "2.564107394218445\n",
      "2.5669809579849243\n",
      "2.5694825649261475\n",
      "[2/10]  Train Loss: 2.428655665665865  Val Loss: 2.570176234841347\n",
      "2.3542761325836183\n",
      "2.354082405567169\n",
      "2.3542733550071717\n",
      "2.3538515329360963\n",
      "2.351799488067627\n",
      "2.3532838940620424\n",
      "2.355102574825287\n",
      "2.354480504989624\n",
      "2.3545336604118345\n",
      "2.3543482065200805\n",
      "2.3521487593650816\n",
      "2.355717384815216\n",
      "2.354356038570404\n",
      "2.355105483531952\n",
      "2.352464699745178\n",
      "2.354747176170349\n",
      "2.353770208358765\n",
      "2.3519059300422667\n",
      "2.3573376059532167\n",
      "2.3551472425460815\n",
      "2.355656659603119\n",
      "2.354658281803131\n",
      "2.3538002490997316\n",
      "2.3537532091140747\n",
      "2.3544562578201296\n",
      "2.3541361689567566\n",
      "2.350209367275238\n",
      "2.3523062229156495\n",
      "2.350916123390198\n",
      "2.353810524940491\n",
      "2.352051866054535\n",
      "2.355924165248871\n",
      "2.356825923919678\n",
      "2.413106679916382\n",
      "2.4484464645385744\n",
      "2.451253867149353\n",
      "2.447420620918274\n",
      "2.4473577618598936\n",
      "2.4473183512687684\n",
      "2.44848552942276\n",
      "2.448956286907196\n",
      "2.4493505716323853\n",
      "2.4493940830230714\n",
      "2.4472745656967163\n",
      "2.4478769063949586\n",
      "2.4496517062187193\n",
      "2.45160973072052\n",
      "2.446529722213745\n",
      "2.4506221413612366\n",
      "2.4477417349815367\n",
      "2.4472705245018007\n",
      "2.4490180015563965\n",
      "2.4508753061294555\n",
      "2.4491931557655335\n",
      "2.4491807341575624\n",
      "2.449869966506958\n",
      "2.448374032974243\n",
      "2.4501779079437256\n",
      "2.4465247869491575\n",
      "2.4468809962272644\n",
      "2.4458622217178343\n",
      "2.4461044907569884\n",
      "2.446773874759674\n",
      "2.4487960934638977\n",
      "2.447840702533722\n",
      "2.4515177965164185\n",
      "2.4839944720268248\n",
      "2.5666680574417113\n",
      "2.5680426716804505\n",
      "2.5656267762184144\n",
      "2.565337634086609\n",
      "2.5620843410491942\n",
      "2.5674811482429503\n",
      "2.5639493584632875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\WORKST~1\\AppData\\Local\\Temp/ipykernel_31152/2224037467.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mpartition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mnist_train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mnist_val'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mplot_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_var2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WORKST~1\\AppData\\Local\\Temp/ipykernel_31152/292560526.py\u001b[0m in \u001b[0;36mexperiment\u001b[1;34m(partition, args)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WORKST~1\\AppData\\Local\\Temp/ipykernel_31152/2949721256.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(encoder, decoder, net, partition, optimizer, criterion, args)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0miter_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmnist_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnist_label\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmnist_train_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0mmnist_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnist_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnist_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \"\"\"\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 1228\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "# ====== Optimizer & Training ====== #\n",
    "args.lr = 0.001\n",
    "args.num_epochs = 200\n",
    "\n",
    "# ====== Model Capacity ===== #\n",
    "args.hidden_dim = 15\n",
    "\n",
    "# ====== Data Loading ====== #\n",
    "args.batch_size = 1\n",
    "args.mnist_batch_size = 512\n",
    "args.num_clients = 3\n",
    "args.num_data = 500\n",
    "args.reverse = False\n",
    "\n",
    "\n",
    "make_dir(args)\n",
    "\n",
    "name_var1 = 'reverse'\n",
    "name_var2 = 'num_data'\n",
    "list_var1 = [False, True]\n",
    "list_var2 = [100, 500, 700, 1000]\n",
    "\n",
    "set, train_dataset, val_dataset = load_data(args)\n",
    "set = set.view(3, -1 ,7851)\n",
    "\n",
    "for var1 in list_var1:\n",
    "    print(f\"========== {name_var1}: {var1} ==========\")\n",
    "    setattr(args, name_var1, var1)\n",
    "    \n",
    "    for var2 in list_var2:\n",
    "        print(f\"[{name_var2}: {var2}]\")\n",
    "        setattr(args, name_var2, var2)\n",
    "\n",
    "        num_set = set[:, torch.randperm(args.num_data), :].reshape(-1, 7851)\n",
    "        print(f\"shape of data: {num_set.shape}\")\n",
    "\n",
    "        train_set = num_set[:int(len(num_set)*0.8)]\n",
    "        val_set = num_set[int(len(num_set)*0.8):]\n",
    "        partition = {'train':train_set, 'val':val_set, 'mnist_train':train_dataset, 'mnist_val':val_dataset}\n",
    "\n",
    "        encoder, decoder, train_losses, val_losses = experiment(partition, deepcopy(args))\n",
    "        plot_loss(var2, name_var2, train_losses, val_losses)\n",
    "        save_model(encoder, decoder, var2, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36b28c691283740da8fac429d25fdbdfa5fc153c522ef0a5c3318a9b3b280dc3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
